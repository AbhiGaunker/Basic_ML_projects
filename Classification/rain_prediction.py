# -*- coding: utf-8 -*-
"""rain_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XxocP4xZ4lCpaBpgXr78UwQWWpc9q7jD

# **Final Assesment **

Getting libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns

"""getting data"""

df = pd.dfs = pd.read_excel("/content/sydney_rain prediction.xlsx")

"""#Preprocessing of the Data"""

df.head()

df.info()

df.describe()

# rainfall is having skeeved dataset

df.corr()

# we should combine MinTemp, MaxTemp, temp3pm and temp9am as it has high covariance -- done in the later part --

sns.jointplot(x='Date', y = 'Rainfall',data = df)

# Since the only location is Sydney we will be removing this feature
sns.countplot(x='Location',data= df)

df = df.drop('Location',axis=1)

sns.countplot(x='RainToday',data= df)

sns.jointplot(x='Date', y = 'Sunshine',data = df)

sns.jointplot(x='Date', y = 'Evaporation',data = df)

sns.countplot(x='RainTomorrow',data= df)

"""### Combining Features (which are correlated to each other) And then visualizing them"""

df['avg_humidity'] = (df.Humidity3pm + df.Humidity9am)/2
df['avg_pressure'] = (df.Pressure3pm +df.Pressure9am)/2
df['avg_cloud'] = (df.Cloud3pm + df.Cloud9am)/2
df['avg_temp'] = (df.Temp3pm + df.Temp9am + df.MaxTemp)/3

df = df.drop(['Humidity3pm','Humidity9am', 'Pressure3pm','Pressure9am','Cloud3pm','Cloud9am','Temp3pm','Temp9am','MaxTemp','MinTemp'],axis=1)

sns.jointplot(x='Date', y = 'avg_humidity',data = df)
sns.jointplot(x='Date', y = 'avg_pressure',data = df)
sns.jointplot(x='Date', y = 'avg_cloud',data = df)
sns.jointplot(x='Date', y = 'avg_temp',data = df)

"""### Dealing with NULL values"""

# we are going to deal with only null values of evaporation and cloud
# and then remove remaining all rows having null data

# for clouds we will be putting group_avg(rain today)
# 1 average of clouds when it rains and another when it doesn't

df['avg_cloud']=df['avg_cloud'].fillna(df.groupby(['RainToday'])['avg_cloud'].transform('mean'))

# mean of evaporation
df['Evaporation']=df['Evaporation'].fillna(df['Evaporation'].mean())

# reamining null data is very few 30-40 which we can drop totally
df=df.dropna()

"""### Creating Dummy variables"""

df = pd.get_dummies(df)

df = df.drop(['RainToday_No','RainTomorrow_No'], axis=1)

df.info()

"""## Outliers

This are all the column which may having outliers:

    rainfall(top), -- confirm that it has outliers as it is skewed dataset --
    avg_temp(top),
    avg_presure(bottom),
    evaporation(top)


"""

np.percentile(df.avg_temp,[99])

uv=np.percentile(df.avg_temp,[99])[0]
df.avg_temp[(df.avg_temp)>3*uv] = 3*uv
lv = np.percentile(df.avg_temp,[1])[0]
df.avg_temp[(df.avg_temp)<0.3*lv] = 0.3*lv

uv=np.percentile(df.avg_pressure,[99])[0]
df.avg_pressure[(df.avg_pressure)>3*uv] = 3*uv
lv = np.percentile(df.avg_pressure,[1])[0]
df.avg_pressure[(df.avg_pressure)<0.3*lv] = 0.3*lv

uv=np.percentile(df.Evaporation,[99])[0]
df.Evaporation   [(df.Evaporation)>3*uv] = 3*uv
lv = np.percentile(df.Evaporation,[1])[0]
df.Evaporation   [(df.Evaporation)<0.3*lv] = 0.3*lv

uv=np.percentile(df.Rainfall,[99])[0]
df.Rainfall[(df.Rainfall)>3*uv] = 3*uv
lv = np.percentile(df.Rainfall,[1])[0]
df.Rainfall[(df.Rainfall)<0.3*lv] = 0.3*lv

# understanding in which case the outliers have changed
# (After Running this ) you can see outlier are mostly present in rainfall dataset not in other dataset
sns.jointplot(x='Date', y = 'Rainfall',data = df)
sns.jointplot(x='Date', y = 'Evaporation',data = df)
sns.jointplot(x='Date', y = 'avg_pressure',data = df)
sns.jointplot(x='Date', y = 'avg_temp',data = df)

# Since all the preprocessing and Data visualization is done we will be removing DATE column
df = df.drop('Date',axis=1)

"""## Spliting the Data"""

x_multi = df.drop('RainTomorrow_Yes',axis=1)
y_multi = df['RainTomorrow_Yes']
x_train,x_test,y_train,y_test = train_test_split(x_multi,y_multi,test_size=0.2,random_state=0)

"""# import necessary libraries"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import confusion_matrix,precision_score,recall_score, roc_auc_score, accuracy_score
from sklearn import tree,preprocessing
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import BaggingClassifier ,RandomForestClassifier,GradientBoostingClassifier

"""#Training Models

## Logistic Regression   (Accuracy: 0.8336)
"""

lr = LogisticRegression()
lr.fit(x_train,y_train)

lr.coef_

lr.intercept_

#statistic model

import statsmodels.api as sn
import statsmodels.discrete.discrete_model as sm

x_cons = sn.add_constant(x_train)
logit = sm.Logit(y_train,x_cons).fit()
logit.summary()

"""### Evaluating"""

# lr.predict_proba(x_test)
y_pred = lr.predict(x_test)

confusion_matrix(y_test,y_pred)

precision_score(y_test,y_pred)

recall_score(y_test,y_pred)

roc_auc_score(y_test,y_pred)

accuracy_score(y_test,y_pred)

"""## LDA (Accuracy: 0.8382)"""

lda = LinearDiscriminantAnalysis()
lda.fit(x_train,y_train)

"""### Evaluating"""

y_pred = lda.predict(x_test)

confusion_matrix(y_test,y_pred)

precision_score(y_test,y_pred)

recall_score(y_test,y_pred)

roc_auc_score(y_test,y_pred)

accuracy_score(y_test,y_pred)

"""## K-nearest Neighbours (Accuracy: 0.83664)"""

# Standardizing the values
scaler = preprocessing.StandardScaler().fit(x_train)
x_train_s = scaler.transform(x_train)
scaler = preprocessing.StandardScaler().fit(x_test)
x_test_s = scaler.transform(x_test)

"""### Getting best model KNN"""

params = {'n_neighbors':[10, 20, 40, 60, 80, 100]}
grid_search = GridSearchCV(KNeighborsClassifier(),params)
grid_search.fit(x_train_s,y_train)

grid_search.best_params_

knn = grid_search.best_estimator_

"""### Evaluating"""

y_pred = knn.predict(x_test_s)

confusion_matrix(y_test,y_pred)

precision_score(y_test,y_pred)

recall_score(y_test,y_pred)

roc_auc_score(y_test,y_pred)

accuracy_score(y_test,y_pred)

"""## Decision tree (Accuracy: 0.82137)

"""

# ctree = tree.DecisionTreeClassifier(max_depth=4)
# ctree.fit(x_train,y_train)     ---we just use gridsearchCV to get the best parameters ---
params = {'max_depth': [1,2,3,4,5,6,7,8,9,10] }
grid_search = GridSearchCV(tree.DecisionTreeClassifier(),params)
grid_search.fit(x_train,y_train)
print(grid_search.best_params_)
ctree = grid_search.best_estimator_

"""### Evaluating"""

y_pred = ctree.predict(x_test)

confusion_matrix(y_test,y_pred)

precision_score(y_test,y_pred)

recall_score(y_test,y_pred)

roc_auc_score(y_test,y_pred)

accuracy_score(y_test,y_pred)

"""## Bagging Classifier (Accuracy: 0.8305)"""

# creating a base model and base estimtor for that model
ctree = tree.DecisionTreeClassifier()
bag = BaggingClassifier(estimator=ctree,
                        bootstrap=True,n_jobs=-1,random_state = 42)

# parameter on which model is to be tested
params = {'n_estimators': [100,200,300,400,500,600,700] }

# getting best bagging model as per pararmeters/ (hyper tuning the model)
grid_search = GridSearchCV(bag,params)
grid_search.fit(x_train,y_train)

# printing the best parameters
print(grid_search.best_params_)
# saving the best model as best_tree
best_tree = grid_search.best_estimator_

"""### Evaluating"""

y_pred = best_tree.predict(x_test)

confusion_matrix(y_test,y_pred)

precision_score(y_test,y_pred)

recall_score(y_test,y_pred)

roc_auc_score(y_test,y_pred)

accuracy_score(y_test,y_pred)

"""## Random forest (Accuracy: 0.84122)"""

rf = RandomForestClassifier(n_estimators=1000,n_jobs=-1,random_state=42)

# parameter on which model is to be tested
params = {'n_estimators': [500,1000],
          "max_features": [4,6,8,10],
          "min_samples_split":[2,3,10]}

# getting best bagging model as per pararmeters / (hyper tuning the model)
grid_search = GridSearchCV(rf,params)
grid_search.fit(x_train,y_train)

# printing the best parameters
print(grid_search.best_params_)
# saving the best model as best_tree
best_tree = grid_search.best_estimator_

"""### Evaluating"""

y_pred = best_tree.predict(x_test)

confusion_matrix(y_test,y_pred)

precision_score(y_test,y_pred)

recall_score(y_test,y_pred)

roc_auc_score(y_test,y_pred)

accuracy_score(y_test,y_pred)

"""## Boosting (Accuracy: 0.82748)"""

gbc = GradientBoostingClassifier()

# parameter on which model is to be tested
params = {"n_estimators": [500,750,1000],
               "learning_rate": [0.05,0.1,0.15],
               "max_depth":[1,2,3,4,5]
               }

# getting best bagging model as per pararmeters / (hyper tuning the model)
grid_search = GridSearchCV(gbc,params)
grid_search.fit(x_train,y_train)
# --- grid_search = GridSearchCV(gbc,params_grid,n_jobs=-1,cv=5,scoring='neg_mean_squared_error')

# printing the best parameters
print(grid_search.best_params_)
# saving the best model as best_tree
best_tree = grid_search.best_estimator_

"""### Evaluating"""

y_pred = best_tree.predict(x_test)

confusion_matrix(y_test,y_pred)

precision_score(y_test,y_pred)

recall_score(y_test,y_pred)

roc_auc_score(y_test,y_pred)

accuracy_score(y_test,y_pred)

# since accuracy is lowwest we wont be going to other bosting methods

"""# Summary

1. Your views about the problem statement?

   * So This is news Paper comapny which want to predit whether it will rain or not the **next day** (as newspaper is distributed early morning so data collected will be of previous day).
   * We are given different data collected at 3 am and 9 pm. We need the statistics for each feature (at least for some common features like temperature and cloud) this is because we can say that if temperature falls below so and so it may rain today or if it remains cloudy will rain.


---


2. What will be your approach to solving this task?

Using the best decision tree and Linear Model to get the required Statistical Data based on accuracy and auc.
For this preprocessing of the data is required

* combinig the features is necessary as they are highly correlated which can lead to multicollinearity or biased model
* Preprocessing the dataset
    * rainfall is skewed feature also after visualization some features appers to have outliers
    * most of the features have null data (cloud and evaporation was filled) remaning was removed


* Divding the dataset into training and Testing
* Training different models on this dataset and fine tuning the models with hyper parameters
*   Finding the best models and getting more details of that models

---

3. What ML model options did you have available to perform this task?

As I have discussed before all regression models can be used in this case. After Preprocessing it is clear that visibly there is no linear correlation but Since we needed statistical data we have used
*   Logisti Regression
*   LDA
*   KNN

In the case of Decision tree we have used
*   regressionDecision
*   Bagging
*   Random forest
*   Boosting

---

4. Which model’s performance is best and what could be the possible reason for that?

It is Randomforest as Decision tree are better than linear_regression models and since Randomforest is an ensemble technique it is better than a simple DecisionTree.

Boosting reduces the bias so when comming to test data it is reducing its performance you can clearly see this as after hyper tuning we are getting higher acuuracy for test dataset compared to randomly choosen data set. thats the reason why we are not going for adaboosting and all

Below are the results for testing Dataset

[RandomForest Model](#scrollTo=BGf0nvE2l4nn&line=1&uniqifier=1) :  { 'max_features': 4, 'min_samples_split': 10, 'n_estimators': 1000 }

*   Accuracy = 0.8412




Liner_models: [LDA](#scrollTo=Bo8WOYzUqfX5&line=1&uniqifier=1)
* r2 value = 0.838


---

5. What steps can you take to improve this selected model’s performance even further?

    * We can further tune this model by trying out different hyper parameters(maximum number of features ,number of estimators,maximum depth of the trees).
    * Train on more data.
    * Evaluate the model with a variety of metrics.
    * Since our priority is predicting rain we can focus on reducing False positive compared to False negative.
    As its ok to sometime give prediction that it may rain even though the chances are less so that people can always take precaitions.
    * adding more features

Stats model is in [Logistic regression](#scrollTo=NVMjaP_aqqR_&line=1&uniqifier=1)
"""

# click on hyper link to go to the models -- it will be the top most model --